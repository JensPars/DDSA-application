<div class="csl-bib-body">
    <div data-csl-entry-id="radford2021learning" class="csl-entry"> [1] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., &#38; others. (2021). Learning transferable visual models from natural language supervision. <i>International Conference on Machine Learning</i>, 8748–8763.</div>
</div>
<div class="csl-bib-body">
    <div data-csl-entry-id="rombach2022high" class="csl-entry">[2] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &#38; Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 10684–10695.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="liu2023visual" class="csl-entry">[3] Liu, H., Li, C., Wu, Q., &#38; Lee, Y. J. (2023). Visual instruction tuning. <i>Advances in Neural Information Processing Systems</i>, <i>36</i>, 34892–34916.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="ren2017personalized" class="csl-entry">[4] Ren, J., Shen, X., Lin, Z., Mech, R., &#38; Foran, D. J. (2017). Personalized image aesthetics. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 638–647.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="yang2022personalized" class="csl-entry">[5] Yang, Y., Xu, L., Li, L., Qie, N., Li, Y., Zhang, P., &#38; Guo, Y. (2022). Personalized image aesthetics assessment with rich attributes. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 19861–19869.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="gal2022image" class="csl-entry">[6] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., &#38; Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion. <i>ArXiv Preprint ArXiv:2208.01618</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="ruiz2023dreambooth" class="csl-entry">[7] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., &#38; Aberman, K. (2023). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 22500–22510.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="sohn2023styledrop" class="csl-entry">[8] Sohn, K., Ruiz, N., Lee, K., Chin, D. C., Blok, I., Chang, H., Barber, J., Jiang, L., Entis, G., Li, Y., &#38; others. (2023). Styledrop: Text-to-image generation in any style. <i>ArXiv Preprint ArXiv:2306.00983</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="von2023fabric" class="csl-entry">[9] Von Rütte, D., Fedele, E., Thomm, J., &#38; Wolf, L. (2023). Fabric: Personalizing diffusion models with iterative feedback. <i>ArXiv Preprint ArXiv:2307.10159</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="salehi2024viper" class="csl-entry">[10] Salehi, S., Shafiei, M., Yeo, T., Bachmann, R., &#38; Zamir, A. (2024). ViPer: Visual Personalization of Generative Models via Individual Preference Learning. <i>European Conference on Computer Vision</i>, 391–406.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="nguyen2025yo" class="csl-entry"[11] >Nguyen, T., Liu, H., Li, Y., Cai, M., Ojha, U., &#38; Lee, Y. J. (2025). Yo’LLaVA: Your Personalized Language and Vision Assistant. <i>Advances in Neural Information Processing Systems</i>, <i>37</i>, 40913–40951.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="yeh2023meta" class="csl-entry">[12] Yeh, C.-H., Russell, B., Sivic, J., Heilbron, F. C., &#38; Jenni, S. (2023). Meta-personalizing vision-language models to find named instances in video. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 19123–19132.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="alaluf2024myvlm" class="csl-entry">[13] Alaluf, Y., Richardson, E., Tulyakov, S., Aberman, K., &#38; Cohen-Or, D. (2024). Myvlm: Personalizing vlms for user-specific queries. <i>European Conference on Computer Vision</i>, 73–91.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="seifi2025personalization" class="csl-entry">[14] Seifi, S., Dorovatas, V., Reino, D. O., &#38; Aljundi, R. (2025). Personalization Toolkit: Training Free Personalization of Large Vision Language Models. <i>ArXiv Preprint ArXiv:2502.02452</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="pi2024personalized" class="csl-entry">[15] Pi, R., Zhang, J., Han, T., Zhang, J., Pan, R., &#38; Zhang, T. (2024). Personalized visual instruction tuning. <i>ArXiv Preprint ArXiv:2410.07113</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="an2024mc" class="csl-entry">[16] An, R., Yang, S., Lu, M., Zeng, K., Luo, Y., Chen, Y., Cao, J., Liang, H., She, Q., Zhang, S., &#38; others. (2024). MC-LLaVA: Multi-Concept Personalized Vision-Language Model. <i>ArXiv Preprint ArXiv:2411.11706</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="pham2024personalized" class="csl-entry">[17] Pham, C., Phan, H., Doermann, D., &#38; Tian, Y. (2024). Personalized Large Vision-Language Models. <i>ArXiv Preprint ArXiv:2412.17610</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="kim2020multimodal" class="csl-entry">Kim, S., Jiang, J.-Y., Nakada, M., Han, J., &#38; Wang, W. (2020). Multimodal Post Attentive Profiling for Influencer Marketing. <i>Proceedings of The Web Conference 2020</i>, 2878–2884.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="delatolas2023eva" class="csl-entry">Delatolas, T., Kalogeiton, V., &#38; Papadopoulos, D. P. (2023). EVA-VOS: Efficient Video Annotation for Video Object Segmentation. <i>ICCVW CVEU</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="parslov2024crashcar101" class="csl-entry">Parslov, J., Riise, E., &#38; Papadopoulos, D. P. (2024). CrashCar101: Procedural Generation for Damage Assessment. <i>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</i>, 4624–4634.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="papadopoulos2017extreme" class="csl-entry">Papadopoulos, D. P., Uijlings, J. R., Keller, F., &#38; Ferrari, V. (2017). Extreme clicking for efficient object annotation. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 4930–4939.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="papadopoulos2019make" class="csl-entry">Papadopoulos, D. P., Tamaazousti, Y., Ofli, F., Weber, I., &#38; Torralba, A. (2019). How to make a pizza: Learning a compositional layer-based gan model. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 8002–8011.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="papadopoulos2022learning" class="csl-entry">Papadopoulos, D. P., Mora, E., Chepurko, N., Huang, K. W., Ofli, F., &#38; Torralba, A. (2022). Learning program representations for food images and cooking recipes. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 16559–16569.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="caron2021emerging" class="csl-entry">Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., &#38; Joulin, A. (2021). Emerging properties in self-supervised vision transformers. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 9650–9660.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="oquab2023dinov2" class="csl-entry">Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., &#38; others. (2023). Dinov2: Learning robust visual features without supervision. <i>ArXiv Preprint ArXiv:2304.07193</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="bardes2022vicregvarianceinvariancecovarianceregularizationselfsupervised" class="csl-entry">Bardes, A., Ponce, J., &#38; LeCun, Y. (2022). <i>VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</i>. ICLR2022</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="Qwen-VL" class="csl-entry">Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., &#38; Zhou, J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. <i>ArXiv Preprint ArXiv:2308.12966</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="hu2022lora" class="csl-entry">Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., &#38; Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. <i>International Conference on Learning Representations</i>. https://openreview.net/forum?id=nZeVKeeFYf9</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="dettmers2023qlora" class="csl-entry">Dettmers, T., Pagnoni, A., Holtzman, A., &#38; Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. <i>ArXiv Preprint ArXiv:2305.14314</i>.</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="vannguyen2021trankitlightweighttransformerbasedtoolkit" class="csl-entry">Nguyen, M. V., Lai, V. D., Veyseh, A. P. B., &#38; Nguyen, T. H. (2021). <i>Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing</i>. https://arxiv.org/abs/2101.03289</div>
  </div>
  <div class="csl-bib-body">
    <div data-csl-entry-id="pmlr-v139-ramesh21a" class="csl-entry">Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., &#38; Sutskever, I. (2021). Zero-Shot Text-to-Image Generation. In M. Meila &#38; T. Zhang (Eds.), <i>Proceedings of the 38th International Conference on Machine Learning</i> (Vol. 139, pp. 8821–8831). PMLR. https://proceedings.mlr.press/v139/ramesh21a.html</div>
  </div>
