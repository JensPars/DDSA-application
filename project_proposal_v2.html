<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PhD Project Proposal</title>
    <style>
        /* Base styles and variables */
        :root {
            --primary-color:  #840000;
            --secondary-color:  #990000;
            --text-color: #333;
            --light-text: #915c54;
            --light-bg: #f8e8e5;
            --accent-color: #e7bbb3;
            --border-color: #f0d1cc;
            --success-color: #34a853;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            color: var(--text-color);
        }
        
        body {
            background-color: #f9f9f9;
            line-height: 1.6;
            padding: 20px;
        }
        
        .proposal-container {
            max-width: 1000px;
            margin: 0 auto;
            background-color: white;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
            padding: 40px;
            border-radius: 10px;
        }
        
        /* Header Styles */
        header {
            text-align: center;
            padding-bottom: 30px;
            border-bottom: 2px solid var(--accent-color);
            margin-bottom: 30px;
        }
        
        header h1 {
            font-size: 2.2rem;
            color: var(--primary-color);
            margin-bottom: 10px;
        }
        
        header p {
            color: var(--light-text);
            font-size: 1.1rem;
        }
        
        /* Section Styling */
        section {
            margin-bottom: 40px;
        }
        
        h2 {
            color: var(--primary-color);
            font-size: 1.7rem;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--accent-color);
            position: relative;
        }
        
        h2::after {
            content: '';
            position: absolute;
            bottom: -2px;
            left: 0;
            width: 100px;
            height: 2px;
            background-color: var(--secondary-color);
        }
        
        h3 {
            font-size: 1.4rem;
            color: var(--secondary-color);
            margin: 20px 0 10px 0;
        }
        
        h4 {
            font-size: 1.2rem;
            color: var(--text-color);
            margin: 15px 0 10px 0;
        }
        
        p {
            margin-bottom: 15px;
        }
        
        /* List styling */
        ul {
            list-style-type: none;
            margin-left: 10px;
            margin-bottom: 15px;
        }
        
        ul li {
            position: relative;
            padding-left: 25px;
            margin-bottom: 10px;
        }
        
        ul li::before {
            content: "â€¢";
            color: var(--secondary-color);
            font-size: 20px;
            position: absolute;
            left: 0;
        }
        
        /* Responsive adjustments */
        @media (max-width: 768px) {
            .proposal-container {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="proposal-container">
        <header>
            <h1>Personalised Vision Models for Subjective Tasks</h1>
            <p>PhD Project Proposal</p>
        </header>

        <section>
            <h2>Background and State of the Art</h2>
            
            <p>
              Computer vision has achieved remarkable success with large foundation models like CLIP [1], Stable Diffusion [2] and LLaVA [3], demonstrating impressive performance across diverse tasks. Developers typically train these models to optimise for a single "ground truth" derived from aggregate annotations, representing an average consensus across many annotators. However, this approach does not address the subjectivity in many visual tasks, where personal preferences and individual perceptions play a crucial role.
            </p>
            
            <p>
                <b>People's preferences</b> vary widely based on culture, background, gender, sex, ethnicity, age, geography, personal experiences, and individual tastes. Models trained on aggregate annotations fail to capture the nuances of individual perception, resulting in suboptimal performance for subjective tasks. As AI systems become more integrated into daily life, personalised models that can adapt to personal preferences and align with user-specific criteria are needed. AI systems that do not account for subjective preferences risk alienating users and reinforcing biases in training data, which can influence individuals' opinions and behaviours, leading to a loss of diversity in opinions and preferences. It is instrumental to develop AI systems that can respect and adapt to individual preferences to <b>(i) ensure performant models</b> and <b>(ii) promote diversity and inclusion in AI applications</b>. or <b>(ii) mitigate the societal risks associated with AI</b>.
            </p>
            <p>
                Several areas in computer vision deal with inherently subjective judgments, including aesthetic evaluation, image and video generation and vision assistants. These applications lack a "correct" answer and depend significantly on individual user preferences and contexts.
            </p>
            
            <p>
                <b>Image aesthetics</b>. Early work addresses the personalisation of image aesthetics. [4,5] gathered datasets with images annotated by subjects across various attributes capturing aesthetics. These methods fail to systematically capture a representative set of preferences across multiple sub-groups.
            </p>
            <p>
                <b>Personalised generation</b> explores generating images containing user-specific concepts and images that align with individual preferences. [6-8] Apply methods to fine-tune models using a few examples. These methods are very compute intensive because users must train the models. 
                [9] utilise liked and disliked reference images. This limited binary data does not capture a general representation of the users' preferences.
                [10] train a general model to map users' free-form comments on generated images to structured visual attributes. Because they lack actual human-annotated data, they synthesise it using VLMs, which leads to the method not being representative of actual human preferences.
                All of the aformentioned methods fail to generalise to concepts not contained in the few training examples and would not capture associations like "likes Harley Davidson motorcycles" -> "likes leather jackets".
            </p>
            <p>    
                <b>Personalised vision language models.</b> [11-17] explore introducing user-specific concepts to vision language models, like my dog, my mum, my friend etc.
                Wei, Tianxin, et al. use VLMs for generative recommendation systems, capturing human preference given some user's interaction history.
                Only one paper addresses the preference personalisation of VLMs, suggesting that more work is needed. Meanwhile LLMs have been personalised 
                <A significant research opportunity exists in developing vision models that can systematically adapt to individual user preferences across a range of subjective tasks, while maintaining the powerful generalization capabilities of foundation models. This requires addressing challenges in efficient personalization, modeling of subjective preferences, and balanced adaptation that preserves model generality.>
            </p>
        </section>

        <section>
            <h2>Research Questions and Methods</h2>
            <p>
            The proposed research aims to develop methods for personalizing large vision models to individual user preferences with minimal user input. Having a general representation of preferences is crucial for personalisation, to this end we propose to build a foundational preference model which can effectively capture user preferences across a range of subjective tasks given very few examples. This leads us to the first research question 
            </p>
            <p>
            <b>RQ1</b>: How can we train a foundational visual preference model?
            </p>
            <p>
            Vision language assistants like LLaVA can be used in everyday life to help users with a range of tasks. Take for example Figure 1, where a used can provide a vision language model with an image of their living room, and ask the model to suggest a new piece of furniture that would fit well in the room, here the preference of the user is crucial. This leads us to the second research question
            </p>
            <p>
            <b>RQ2</b>: How can we make personalisable vision langauge Model?
            </p>
            <p>
            Image generation models can be used to create personalised content for users. For example, a user could provide a few examples of images they like, and the model could generate new images that align with the user's preferences. This leads us to the third research question
            </p>
            <b>RQ3</b>: How can we make personalisable image generation Model? 

            <h4>Methodology</h4>
            <p>
                To address <b>RQ1</b>, we propose developing a foundational preference model trained on web-scale images and associated metadata [18]. Traditional approaches requiring human-annotated preference data are prohibitively expensive and time-consuming at scale. Instead, we leverage social media content where influencers and users naturally curate images reflecting their preferences.
                Our approach capitalises on the implicit preference signals in user posting behaviour; images shared by the same user or within communities likely represent coherent preference patterns. Using self-supervised learning methods building on recent advances in visual representation learning e.g. [1,24-26], we aim to construct an embedding space where collections of images are matched if they exhibit similar characteristics that correlate with user preferences.
                This embedding space enables mapping collections of images to preference vectors, creating a foundation for personalised visual recommendation systems without requiring explicit preference annotations.
            </p>
            <p>
                To address <b>RQ2</b>, we propose extending the foundational preference model to a vision language model that can answer user-specific queries about visual content. We seek to adapt multimodal language models e.g. [3, 27] with user-specific preference embeddings, enabling the model to generate responses that align with individual user preferences. LoRA, QLoRA, adapters etc. [28-30]. Addtionally we will explore techniques for continual learning to adapt the model to new user preferences over time.
            </p>
            <p>
                To address <b>RQ3</b>, we propose developing a personalisable image generation model to create new images based on user preferences. We aim to leverage recent advances in generative models e.g. [2, 31] to generate images that align with user-specific preference vectors. We will explore techniques for conditioning generative models on preference embeddings, enabling users to guide the image-generation process with minimal examples. Additionally, building on recent work in personalisation of generative models [10], we aim to generate a synthetic dataset of prefered user profile image pairs to train the model.
            </p>
        </section>

        <section>
            <h2>Outcome and Potential Impact</h2>
            <p>
                This research will be published in top-tier venues like CVPR, ICCV, NeurIPS, and ICML, with open-source code, models, and datasets available for the community. We will share findings through workshops and talks at key conferences, collaborating with academic and industry partners for broader impact.
            </p>
                
            <h3>Methodological Contributions</h3>
            <p>
                We will introduce a Preference Foundation Model to learn user preference embeddings from web-scale image data without explicit annotations, allowing for few-shot adaptation. Additionally, we'll develop a parameter-efficient adaptation method for large vision-language models and create personalisable image generation models.
            </p>
                
            <h3>Broader Impact</h3>
            <p>
                This research aims to improve AI systems' adaptation to human preferences, enhancing inclusivity and accessibility. Applications include personalised educational materials, patient-centric communication in healthcare, and tools for creative industries and e-commerce, ultimately improving user satisfaction.
            </p>
                
            <p>
                By addressing subjective preferences, we strive for AI that supports human diversity and autonomy, leading to more inclusive and human-centred systems across various domains.
            </p>
           
        </section>



    </div>


    
</body>
</html>