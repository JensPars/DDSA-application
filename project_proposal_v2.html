<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PhD Project Proposal</title>
    <style>
        /* Base styles and variables */
        :root {
            --primary-color:  #840000;
            --secondary-color:  #990000;
            --text-color: #333;
            --light-text: #915c54;
            --light-bg: #f8e8e5;
            --accent-color: #e7bbb3;
            --border-color: #f0d1cc;
            --success-color: #34a853;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            color: var(--text-color);
        }
        
        body {
            background-color: #f9f9f9;
            line-height: 1.6;
            padding: 20px;
        }
        
        .proposal-container {
            max-width: 1000px;
            margin: 0 auto;
            background-color: white;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
            padding: 40px;
            border-radius: 10px;
        }
        
        /* Header Styles */
        header {
            text-align: center;
            padding-bottom: 30px;
            border-bottom: 2px solid var(--accent-color);
            margin-bottom: 30px;
        }
        
        header h1 {
            font-size: 2.2rem;
            color: var(--primary-color);
            margin-bottom: 10px;
        }
        
        header p {
            color: var(--light-text);
            font-size: 1.1rem;
        }
        
        /* Section Styling */
        section {
            margin-bottom: 40px;
        }
        
        h2 {
            color: var(--primary-color);
            font-size: 1.7rem;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--accent-color);
            position: relative;
        }
        
        h2::after {
            content: '';
            position: absolute;
            bottom: -2px;
            left: 0;
            width: 100px;
            height: 2px;
            background-color: var(--secondary-color);
        }
        
        h3 {
            font-size: 1.4rem;
            color: var(--secondary-color);
            margin: 20px 0 10px 0;
        }
        
        h4 {
            font-size: 1.2rem;
            color: var(--text-color);
            margin: 15px 0 10px 0;
        }
        
        p {
            margin-bottom: 15px;
        }
        
        /* List styling */
        ul {
            list-style-type: none;
            margin-left: 10px;
            margin-bottom: 15px;
        }
        
        ul li {
            position: relative;
            padding-left: 25px;
            margin-bottom: 10px;
        }
        
        ul li::before {
            content: "•";
            color: var(--secondary-color);
            font-size: 20px;
            position: absolute;
            left: 0;
        }
        
        /* Responsive adjustments */
        @media (max-width: 768px) {
            .proposal-container {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="proposal-container">
        <header>
            <h1>Personalised Vision Models for Subjective Tasks</h1>
            <p>PhD Project Proposal</p>
        </header>

        <section>
            <h2>Background and State of the Art</h2>
            
            <p>
              Computer vision has achieved remarkable success with large foundation models like CLIP [1], Stable Diffusion [2] and LLaVA [3], demonstrating impressive performance across diverse tasks. Developers typically train these models to optimise for a single "ground truth" derived from aggregate annotations, representing an average consensus across many annotators. However, this approach does not address the subjectivity in many visual tasks, where personal preferences and individual perceptions play a crucial role.
            </p>
            
            <p>
                <b>People's preferences</b> vary widely based on culture, background, gender, sex, ethnicity, age, geography, personal experiences, and individual tastes. Models trained on aggregate annotations fail to capture the nuances of individual perception, resulting in suboptimal performance for subjective tasks. As AI systems become more integrated into daily life, personalised models that can adapt to personal preferences and align with user-specific criteria are needed. AI systems that do not account for subjective preferences risk alienating users and reinforcing biases in training data, which can influence individuals' opinions and behaviours, leading to a loss of diversity in opinions and preferences. It is instrumental to develop AI systems that can respect and adapt to individual preferences to <b>(i) ensure performant models</b> and <b>(ii) promote diversity and inclusion in AI applications</b>. or <b>(ii) mitigate the societal risks associated with AI</b>.
            </p>
            <p>
                Several areas in computer vision deal with inherently subjective judgments, including aesthetic evaluation, image and video generation and vision assistants. These applications lack a "correct" answer and depend significantly on individual user preferences and contexts.
            </p>
            
            <p>
                <b>Image aesthetics</b>. Early work addresses the personalisation of image aesthetics. [4,5] gathered datasets with images annotated by subjects across various attributes capturing aesthetics. These methods fail to systematically capture a representative set of preferences across multiple sub-groups.
            </p>
            <p>
                <b>Personalised generation</b> explores generating images containing user-specific concepts and images that align with individual preferences. [6-8] Apply methods to fine-tune models using a few examples. These methods are very compute intensive because users must train the models. 
                [9] utilise liked and disliked reference images. This limited binary data does not capture a general representation of the users' preferences.
                [10] train a general model to map users' free-form comments on generated images to structured visual attributes. Because they lack actual human-annotated data, they synthesise it using VLMs, which leads to the method not being representative of actual human preferences.
                All of the aformentioned methods fail to generalise to concepts not contained in the few training examples and would not capture associations like "likes Harley Davidson motorcycles" -> "likes leather jackets".
            </p>
            <p>    
                <b>Personalised vision language models.</b> [11-17] explore introducing user-specific concepts to vision language models, like my dog, my mum, my friend etc.
                Wei, Tianxin, et al. use VLMs for generative recommendation systems, capturing human preference given some user's interaction history.
                Only one paper addresses the preference personalisation of VLMs, suggesting that more work is needed. Meanwhile LLMs have been personalised 
                <A significant research opportunity exists in developing vision models that can systematically adapt to individual user preferences across a range of subjective tasks, while maintaining the powerful generalization capabilities of foundation models. This requires addressing challenges in efficient personalization, modeling of subjective preferences, and balanced adaptation that preserves model generality.>
            </p>
        </section>

        <section>
            <h2>Research Questions and Methods</h2>
            <p>
            The proposed research aims to develop methods for personalizing large vision models to individual user preferences with minimal user input. Having a general representation of preferences is crucial for personalisation, to this end we propose to build a foundational preference model which can effectively capture user preferences across a range of subjective tasks given very few examples. This leads us to the first research question 
            </p>
            <p>
            <b>RQ1</b>: How can we train a foundational visual preference model?
            </p>
            <p>
            Vision language assistants like LLaVA can be used in everyday life to help users with a range of tasks. Take for example Figure 1, where a used can provide a vision language model with an image of their living room, and ask the model to suggest a new piece of furniture that would fit well in the room, here the preference of the user is crucial. This leads us to the second research question
            </p>
            <p>
            <b>RQ2</b>: How can we make personalisable vision langauge Model?
            </p>
            <p>
            Image generation models can be used to create personalised content for users. For example, a user could provide a few examples of images they like, and the model could generate new images that align with the user's preferences. This leads us to the third research question
            </p>
            <b>RQ3</b>: How can we make personalisable image generation Model? 

            <h4>Methodology</h4>
            <p>
                To address <b>RQ1</b>, we propose developing a foundational preference model trained on web-scale images and associated metadata [18]. Traditional approaches requiring human-annotated preference data are prohibitively expensive and time-consuming at scale. Instead, we leverage social media content where influencers and users naturally curate images reflecting their preferences.
                Our approach capitalises on the implicit preference signals in user posting behaviour; images shared by the same user or within communities likely represent coherent preference patterns. Using self-supervised learning methods building on recent advances in visual representation learning e.g. [1,24-26], we aim to construct an embedding space where collections of images are matched if they exhibit similar characteristics that correlate with user preferences.
                This embedding space enables mapping collections of images to preference vectors, creating a foundation for personalised visual recommendation systems without requiring explicit preference annotations.
            </p>
            <p>
                To address <b>RQ2</b>, we propose extending the foundational preference model to a vision language model that can answer user-specific queries about visual content. We seek to adapt multimodal language models e.g. [3, 27] with user-specific preference embeddings, enabling the model to generate responses that align with individual user preferences. LoRA, QLoRA, adapters etc. [28-30]. Addtionally we will explore techniques for continual learning to adapt the model to new user preferences over time.
            </p>
            <p>
                To address <b>RQ3</b>, we propose developing a personalisable image generation model to create new images based on user preferences. We aim to leverage recent advances in generative models e.g. [2, 31] to generate images that align with user-specific preference vectors. We will explore techniques for conditioning generative models on preference embeddings, enabling users to guide the image-generation process with minimal examples. Additionally, building on recent work in personalisation of generative models [10], we aim to generate a synthetic dataset of prefered user profile image pairs to train the model.
            </p>
        </section>

        <section>
            <h2>Outcome and Potential Impact</h2>
            <p>
                This research will be published in top-tier venues like CVPR, ICCV, NeurIPS, and ICML, with open-source code, models, and datasets available for the community. We will share findings through workshops and talks at key conferences, collaborating with academic and industry partners for broader impact.
            </p>
                
            <h3>Methodological Contributions</h3>
            <p>
                We will introduce a Preference Foundation Model to learn user preference embeddings from web-scale image data without explicit annotations, allowing for few-shot adaptation. Additionally, we'll develop a parameter-efficient adaptation method for large vision-language models and create personalisable image generation models.
            </p>
                
            <h3>Broader Impact</h3>
            <p>
                This research aims to improve AI systems' adaptation to human preferences, enhancing inclusivity and accessibility. Applications include personalised educational materials, patient-centric communication in healthcare, and tools for creative industries and e-commerce, ultimately improving user satisfaction.
            </p>
                
            <p>
                By addressing subjective preferences, we strive for AI that supports human diversity and autonomy, leading to more inclusive and human-centred systems across various domains.
            </p>
           
        </section>



    </div>

    <div class="csl-bib-body">
        <div data-csl-entry-id="radford2021learning" class="csl-entry"> [1] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., &#38; others. (2021). Learning transferable visual models from natural language supervision. <i>International Conference on Machine Learning</i>, 8748–8763.</div>
    </div>
    <div class="csl-bib-body">
        <div data-csl-entry-id="rombach2022high" class="csl-entry">[2] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &#38; Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 10684–10695.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="liu2023visual" class="csl-entry">[3] Liu, H., Li, C., Wu, Q., &#38; Lee, Y. J. (2023). Visual instruction tuning. <i>Advances in Neural Information Processing Systems</i>, <i>36</i>, 34892–34916.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="ren2017personalized" class="csl-entry">[4] Ren, J., Shen, X., Lin, Z., Mech, R., &#38; Foran, D. J. (2017). Personalized image aesthetics. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 638–647.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="yang2022personalized" class="csl-entry">[5] Yang, Y., Xu, L., Li, L., Qie, N., Li, Y., Zhang, P., &#38; Guo, Y. (2022). Personalized image aesthetics assessment with rich attributes. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 19861–19869.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="gal2022image" class="csl-entry">[6] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., &#38; Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion. <i>ArXiv Preprint ArXiv:2208.01618</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="ruiz2023dreambooth" class="csl-entry">[7] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., &#38; Aberman, K. (2023). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 22500–22510.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="sohn2023styledrop" class="csl-entry">[8] Sohn, K., Ruiz, N., Lee, K., Chin, D. C., Blok, I., Chang, H., Barber, J., Jiang, L., Entis, G., Li, Y., &#38; others. (2023). Styledrop: Text-to-image generation in any style. <i>ArXiv Preprint ArXiv:2306.00983</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="von2023fabric" class="csl-entry">[9] Von Rütte, D., Fedele, E., Thomm, J., &#38; Wolf, L. (2023). Fabric: Personalizing diffusion models with iterative feedback. <i>ArXiv Preprint ArXiv:2307.10159</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="salehi2024viper" class="csl-entry">[10] Salehi, S., Shafiei, M., Yeo, T., Bachmann, R., &#38; Zamir, A. (2024). ViPer: Visual Personalization of Generative Models via Individual Preference Learning. <i>European Conference on Computer Vision</i>, 391–406.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="nguyen2025yo" class="csl-entry">[11] Nguyen, T., Liu, H., Li, Y., Cai, M., Ojha, U., &#38; Lee, Y. J. (2025). Yo’LLaVA: Your Personalized Language and Vision Assistant. <i>Advances in Neural Information Processing Systems</i>, <i>37</i>, 40913–40951.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="yeh2023meta" class="csl-entry">[12] Yeh, C.-H., Russell, B., Sivic, J., Heilbron, F. C., &#38; Jenni, S. (2023). Meta-personalizing vision-language models to find named instances in video. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 19123–19132.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="alaluf2024myvlm" class="csl-entry">[13] Alaluf, Y., Richardson, E., Tulyakov, S., Aberman, K., &#38; Cohen-Or, D. (2024). Myvlm: Personalizing vlms for user-specific queries. <i>European Conference on Computer Vision</i>, 73–91.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="seifi2025personalization" class="csl-entry">[14] Seifi, S., Dorovatas, V., Reino, D. O., &#38; Aljundi, R. (2025). Personalization Toolkit: Training Free Personalization of Large Vision Language Models. <i>ArXiv Preprint ArXiv:2502.02452</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="pi2024personalized" class="csl-entry">[15] Pi, R., Zhang, J., Han, T., Zhang, J., Pan, R., &#38; Zhang, T. (2024). Personalized visual instruction tuning. <i>ArXiv Preprint ArXiv:2410.07113</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="an2024mc" class="csl-entry">[16] An, R., Yang, S., Lu, M., Zeng, K., Luo, Y., Chen, Y., Cao, J., Liang, H., She, Q., Zhang, S., &#38; others. (2024). MC-LLaVA: Multi-Concept Personalized Vision-Language Model. <i>ArXiv Preprint ArXiv:2411.11706</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="pham2024personalized" class="csl-entry">[17] Pham, C., Phan, H., Doermann, D., &#38; Tian, Y. (2024). Personalized Large Vision-Language Models. <i>ArXiv Preprint ArXiv:2412.17610</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="kim2020multimodal" class="csl-entry">[18] Kim, S., Jiang, J.-Y., Nakada, M., Han, J., &#38; Wang, W. (2020). Multimodal Post Attentive Profiling for Influencer Marketing. <i>Proceedings of The Web Conference 2020</i>, 2878–2884.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="delatolas2023eva" class="csl-entry">[19] Delatolas, T., Kalogeiton, V., &#38; Papadopoulos, D. P. (2023). EVA-VOS: Efficient Video Annotation for Video Object Segmentation. <i>ICCVW CVEU</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="parslov2024crashcar101" class="csl-entry">[20] Parslov, J., Riise, E., &#38; Papadopoulos, D. P. (2024). CrashCar101: Procedural Generation for Damage Assessment. <i>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</i>, 4624–4634.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="papadopoulos2017extreme" class="csl-entry">[21] Papadopoulos, D. P., Uijlings, J. R., Keller, F., &#38; Ferrari, V. (2017). Extreme clicking for efficient object annotation. <i>Proceedings of the IEEE International Conference on Computer Vision</i>, 4930–4939.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="papadopoulos2019make" class="csl-entry">[22] Papadopoulos, D. P., Tamaazousti, Y., Ofli, F., Weber, I., &#38; Torralba, A. (2019). How to make a pizza: Learning a compositional layer-based gan model. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 8002–8011.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="papadopoulos2022learning" class="csl-entry">[23] Papadopoulos, D. P., Mora, E., Chepurko, N., Huang, K. W., Ofli, F., &#38; Torralba, A. (2022). Learning program representations for food images and cooking recipes. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, 16559–16569.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="caron2021emerging" class="csl-entry">[24] Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., &#38; Joulin, A. (2021). Emerging properties in self-supervised vision transformers. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i>, 9650–9660.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="oquab2023dinov2" class="csl-entry">[25] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., &#38; others. (2023). Dinov2: Learning robust visual features without supervision. <i>ArXiv Preprint ArXiv:2304.07193</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="bardes2022vicregvarianceinvariancecovarianceregularizationselfsupervised" class="csl-entry">[26] Bardes, A., Ponce, J., &#38; LeCun, Y. (2022). <i>VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</i>. ICLR2022</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="Qwen-VL" class="csl-entry">[27] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., &#38; Zhou, J. (2023). Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. <i>ArXiv Preprint ArXiv:2308.12966</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="hu2022lora" class="csl-entry">[28] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., &#38; Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. <i>International Conference on Learning Representations</i>. https://openreview.net/forum?id=nZeVKeeFYf9</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="dettmers2023qlora" class="csl-entry">[29] Dettmers, T., Pagnoni, A., Holtzman, A., &#38; Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. <i>ArXiv Preprint ArXiv:2305.14314</i>.</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="vannguyen2021trankitlightweighttransformerbasedtoolkit" class="csl-entry">[30] Nguyen, M. V., Lai, V. D., Veyseh, A. P. B., &#38; Nguyen, T. H. (2021). <i>Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing</i>. https://arxiv.org/abs/2101.03289</div>
      </div>
      <div class="csl-bib-body">
        <div data-csl-entry-id="pmlr-v139-ramesh21a" class="csl-entry">[31] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., &#38; Sutskever, I. (2021). Zero-Shot Text-to-Image Generation. In M. Meila &#38; T. Zhang (Eds.), <i>Proceedings of the 38th International Conference on Machine Learning</i> (Vol. 139, pp. 8821–8831). PMLR. https://proceedings.mlr.press/v139/ramesh21a.html</div>
      </div>
    
    
</body>
</html>